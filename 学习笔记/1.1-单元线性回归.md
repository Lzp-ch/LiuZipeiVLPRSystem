### ex1

背景：Suppose you are the CEO of a restaurant franchise and are considering diﬀerent cities for opening a new outlet. The chain already has trucks in various cities and you have data for proﬁts and populations from the cities.

#### 载入初始化数据

~~~ octave
data = load ('ex1data1.txt')%在文件的路径下，读取数据并存入data
x = data (:, 1);%赋值为data的第一列数据，:写成1也可以，跟维度有关
y = data (:, 2);%赋值为data的第二列数据
m = length(y);%训练集个数m
~~~

ex1data1.txt的格式如下

~~~ 
6.1101,17.592
5.5277,9.1302
8.5186,13.662
7.0032,11.854
5.8598,6.8233
8.3829,11.886
7.4764,4.3483
~~~

#### 数据可视化

~~~ octave
plot(x, y, 'rx', 'MarkerSize', 10);
%r代表坐标系点的颜色是红色，x代表坐标系点是x，后边两个参数表示坐标系中点的大小
ylabel('Profit in $10,000s'); 
%给y轴增加标签说明
xlabel('Population of City in 10,000s');
~~~

#### 准备梯度下降算法

- 代价函数$J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$
- 预测函数$h_\theta(x) = \theta^Tx = \theta_0+\theta_1x$
- 梯度下降更新$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$

~~~ octave
X = [ones(m, 1), data(:,1)]; % 在原有x向量前加一列1，组成x矩阵，为了实现预测函数向量化矩阵相乘 
theta = zeros(2, 1); %初始化θ矩阵，两行一列
iterations = 1500; %梯度下降的迭代次数
alpha = 0.01;%设置学习速率为0.01
~~~

- 测试代价函数计算

~~~ octave
J = computeCost(x, y, theta);%以目前的一组θ值(0,0)计算一下代价函数
~~~

##### computeCost.m函数

~~~ octave
function J = computeCost(X, y, theta)
%x是一个矩阵，y是一个向量，theta是一个向量
%COMPUTECOST Compute cost for linear regression
%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the
%   parameter for linear regression to fit the data points in X and y

% Initialize some useful values
%初始化需要的值
m = length(y); % number of training examples %训练集个数

% You need to return the following variables correctly 
J = 0;
 
loss = sum((theta'*X' - y').^2);
%θ的转置*x的转置，结果是一个向量，1*m维度;
%y的转置，1*m维度，相减还是1*m维度；
%.^2 可以实现将矩阵每个元素分别求平方；
%sum函数参数是矩阵时，可以做到把矩阵每个元素都求和；
%最后结果是一个实数
%J的后半部分求出来了

J = 1/(2*m)*loss;
%前半部分

% =========================================================================

end
~~~



#### 运行梯度下降算法

~~~ octave
theta = gradientDescent(X, y, theta, alpha, iterations);
~~~

##### gradientDescent函数

~~~octave
function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters

    theta = theta  -  alpha * (1 / m) * X'*(X*theta - y);
    %X是m*2维，theta是2*1维，结果是m*1维，减去y也是m*1维
    %x'是2*m维 最后结果是一个实数

    % ============================================================

    % Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);

end

end
~~~

#### 得到θ可视化拟合直线h

~~~ octave
hold on %保持刚刚画的坐标点
plot(X(:,2), X*theta, '-') %取x矩阵的第二列作为横坐标，因为x第一列是1，X*theta是h的值，-是直线风格
legend('Training data', 'Linear regression')%给他们加上相应的文字说明，按照画图顺序排列
~~~

#### 可视化J(theta_0, theta_1)

~~~ octave
% Grid over which we will calculate J
theta0_vals = linspace(-10, 10, 100);
%设置图像中theta的值域
theta1_vals = linspace(-1, 4, 100);

% initialize J_vals to a matrix of 0's
J_vals = zeros(length(theta0_vals), length(theta1_vals));

% Fill out J_vals
for i = 1:length(theta0_vals)
    for j = 1:length(theta1_vals)
	  t = [theta0_vals(i); theta1_vals(j)];
	  J_vals(i,j) = computeCost(X, y, t);
    end
end


% Because of the way meshgrids work in the surf command, we need to
% transpose J_vals before calling surf, or else the axes will be flipped
J_vals = J_vals';
% Surface plot
figure;
surf(theta0_vals, theta1_vals, J_vals)
xlabel('\theta_0'); ylabel('\theta_1');

% Contour plot
figure;
% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100
contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))
xlabel('\theta_0'); ylabel('\theta_1');
hold on;
plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2);

~~~

